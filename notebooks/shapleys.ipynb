{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import Trainable\n",
    "from src.sverl import Shapley\n",
    "import torch\n",
    "import os\n",
    "from src.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'breakout': {\n",
    "        'policy': [\n",
    "            lambda : Trainable.load_checkpoint(\"checkpoints\\\\breakout-ppo-ps-1-1744764661\\\\100.pt\"),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\Breakout PPO PS Separated Actions-1744953301\\\\100.pt'),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\Breakout PPO PS Cross-entropy\\\\400.pt'),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\breakout-ppo-ps-final\\\\400.pt'),\n",
    "        ],\n",
    "        'value': [\n",
    "            lambda : Trainable.load_checkpoint(\"checkpoints\\\\breakout-ppo-vs-1-1744770258\\\\50.pt\"),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\breakout-ppo-vs-final-1\\\\400.pt'),\n",
    "        ]\n",
    "    },\n",
    "    'pong': {\n",
    "        'policy': [\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\pong-ppo-ps-final-1\\\\400.pt'),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\pong-ppo-ps-final-2\\\\400.pt'),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\pong-ppo-ps-final-3\\\\400.pt'),\n",
    "        ],\n",
    "        'value': [\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\pong-ppo-vs-final-1\\\\400.pt'),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\pong-ppo-vs-final-2\\\\400.pt'),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\pong-ppo-vs-final-3\\\\400.pt'),\n",
    "        ]\n",
    "    },\n",
    "    'invaders': {\n",
    "        'policy': [\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\invaders-ppo-ps-final-1\\\\400.pt'),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\invaders-ppo-ps-final-2\\\\400.pt'),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\invaders-ppo-ps-final-3\\\\400.pt'),\n",
    "        ],\n",
    "        'value': [\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\invaders-ppo-vs-final-1\\\\400.pt'),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\invaders-ppo-vs-final-2\\\\400.pt'),\n",
    "            lambda : Trainable.load_checkpoint('checkpoints\\\\invaders-ppo-vs-final-3\\\\400.pt'),\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV: str = 'breakout'\n",
    "TYPE: str = 'policy'\n",
    "INDEX: int = 2\n",
    "\n",
    "SHAPLEY_THRESHOLD: int = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben Rall\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\Coding\\Year 3 Project\\logs folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Ben Rall\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\Coding\\Year 3 Project\\logs folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "models: list[Shapley] = MODELS[ENV][TYPE]\n",
    "model: Shapley = models[INDEX]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False\n",
    "\n",
    "if TEST:\n",
    "    # Measures validation loss\n",
    "    total_loss = 0\n",
    "    N = 5000\n",
    "    for x in model.state_sampler.sample(N):\n",
    "        # Applies the mask to the state observation\n",
    "        mask = torch.rand(x.shape) < 0.5\n",
    "        model.characteristic.masker.mask(x, mask)\n",
    "        masked = model.characteristic.masker.masked_like(x)\n",
    "\n",
    "        # Calculates the relevant characteristic function results \n",
    "        part_1 = model.characteristic.infer(x)\n",
    "        part_2 = model.characteristic.infer(masked)\n",
    "\n",
    "        # Gets the relevant outputs from the shapley model\n",
    "        with torch.no_grad():\n",
    "            results = model.model(x)\n",
    "            masked_results = mask.unsqueeze(-1).to(model.device) * results\n",
    "            dim = tuple(range(1, masked_results.dim() - 1))\n",
    "            part_3 = masked_results.sum(dim=dim)\n",
    "\n",
    "            # Calculates MSE loss\n",
    "            total_loss += torch.square(part_1 - part_2 - part_3).sum().item()\n",
    "\n",
    "    # Calculates and logs the average validation loss\n",
    "    total_loss /= N\n",
    "    print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 1744679029\n",
      "Loaded: 1744679030\n",
      "Loaded: 1744679036\n",
      "Loaded: 1744679056\n",
      "Loaded: 1744679057\n",
      "Loaded: 1744679058\n",
      "Loaded: 1744679061\n",
      "Loaded: 1744679064\n",
      "Loaded: 1744679065\n",
      "Loaded: 1744679071\n",
      "Loaded: 1744679072\n",
      "Loaded: 1744679074\n",
      "Loaded: 1744679096\n",
      "Loaded: 1744679100\n",
      "Loaded: 1744679109\n",
      "Loaded: 1744679115\n",
      "Loaded: 1744679121\n",
      "Loaded: 1744679122\n",
      "Loaded: 1744679127\n",
      "Loaded: 1744679130\n",
      "Loaded: 1744679143\n",
      "Loaded: 1744679148\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "import numpy as np\n",
    "def load_test_data(folder_path):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            data.append(torch.load(full_path, weights_only=False))\n",
    "            print(f\"Loaded: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {filename}: {e}\")\n",
    "    return data\n",
    "\n",
    "folder = \"./data/test/\" + ENV\n",
    "test_data = load_test_data(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import cv2\n",
    "def symmetric_percentile_clip_and_normalize(shapley_vals, p=2):\n",
    "    \"\"\"\n",
    "    Clips and normalizes Shapley values symmetrically based on percentiles.\n",
    "    \n",
    "    Args:\n",
    "        shapley_vals: np.ndarray of shape (H, W, A) â€“ Shapley values for each action.\n",
    "        lower: Lower percentile (e.g., 1).\n",
    "        upper: Upper percentile (e.g., 99).\n",
    "\n",
    "    Returns:\n",
    "        Normalized Shapley values in range [-1, 1], same shape as input.\n",
    "    \"\"\"\n",
    "    # Compute symmetric percentile clipping threshold\n",
    "    abs_vals = np.abs(shapley_vals)\n",
    "    threshold = np.percentile(abs_vals, p)  # same for both sides\n",
    "\n",
    "    # Clip symmetrically\n",
    "    clipped = np.clip(shapley_vals, -threshold, threshold)\n",
    "\n",
    "    # Normalize to [-1, 1]\n",
    "    normalized = clipped / threshold\n",
    "    return normalized\n",
    "\n",
    "def shapley_to_rgba_overlay(shapley_values: torch.Tensor) -> list:\n",
    "    \"\"\"\n",
    "    Converts a (4, 84, 84, 4) tensor of Shapley values into a list of RGBA overlays\n",
    "    (one for each of the 4 actions), resized to (160, 210).\n",
    "    \"\"\"\n",
    "    shapley_values = shapley_values.squeeze(2)  # (4, 84, 84, 4)\n",
    "    shapley_per_action = shapley_values.permute(3, 0, 1, 2)  # (action, frames, H, W)\n",
    "    overlays = []\n",
    "\n",
    "    shapley_per_action = shapley_per_action.sum(dim=1)\n",
    "    abs_vals = shapley_per_action.abs().cpu().numpy()\n",
    "    threshold = np.percentile(abs_vals, SHAPLEY_THRESHOLD)\n",
    "    clipped = torch.clip(shapley_per_action, -threshold, threshold)\n",
    "    normalised_shapley_per_action = clipped / threshold\n",
    "\n",
    "\n",
    "    for a in range(normalised_shapley_per_action.shape[0]):\n",
    "        # norm = np.max(np.abs(values)) + 1e-8  # to avoid division by zero\n",
    "        \n",
    "        # values = np.clip(values / norm, -1, 1)\n",
    "        # values -= values.mean()\n",
    "        # values /= values.std()\n",
    "        values = normalised_shapley_per_action[a].cpu().numpy()\n",
    "        # values = normalised_shapley_per_action[a].cpu().numpy()\n",
    "        # values -= values.mean()\n",
    "        # values /= values.std()\n",
    "        # Create heatmap\n",
    "        rgba = np.zeros((84, 84, 4), dtype=np.uint8)\n",
    "        pos_mask = values > 0\n",
    "        neg_mask = values < 0\n",
    "        rgba[:, :, 0] = (255 * (values * pos_mask)).astype(np.uint8)   # Red\n",
    "        rgba[:, :, 2] = (255 * (-values * neg_mask)).astype(np.uint8)  # Blue\n",
    "        rgba[:, :, 3] = 55 + (200 * np.abs(values)).astype(np.uint8)        # Alpha\n",
    "\n",
    "        # Resize to (160, 210) and transpose to match Pygame format\n",
    "        # rgba_resized = cv2.resize(rgba, (210, 160), interpolation=cv2.INTER_CUBIC)\n",
    "        overlays.append(rgba)\n",
    "\n",
    "    return overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_overlay(overlay_rgba: np.ndarray, background_rgb: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Blends an RGBA overlay onto an RGB background using alpha compositing.\n",
    "    \n",
    "    Args:\n",
    "        overlay_rgba (np.ndarray): (H, W, 4) RGBA overlay image.\n",
    "        background_rgb (np.ndarray): (H, W, 3) RGB background image.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: (H, W, 3) blended RGB image.\n",
    "    \"\"\"\n",
    "    # Ensure values are float32 in range [0, 1]\n",
    "    overlay_rgba = overlay_rgba.astype(np.float32) / 255.0\n",
    "    background_rgb = background_rgb.astype(np.float32) / 255.0\n",
    "\n",
    "    # Split RGBA channels\n",
    "    overlay_rgb = overlay_rgba[..., :3]\n",
    "    alpha = overlay_rgba[..., 3:]\n",
    "\n",
    "    # Alpha blend: out = overlay * alpha + background * (1 - alpha)\n",
    "    blended = overlay_rgb * alpha + background_rgb * (1.0 - alpha)\n",
    "\n",
    "    # Clip and convert back to uint8\n",
    "    blended = np.clip(blended * 255, 0, 255).astype(np.uint8)\n",
    "    return blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0') tensor([[0.4422, 0.3211, 0.1897, 0.0470]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(3, device='cuda:0') tensor([[8.5130e-02, 8.2029e-02, 2.1193e-06, 8.3284e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(3, device='cuda:0') tensor([[5.6505e-07, 1.4355e-05, 4.3161e-09, 9.9999e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(1, device='cuda:0') tensor([[2.8190e-01, 7.1808e-01, 7.6883e-06, 9.6580e-06]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(2, device='cuda:0') tensor([[5.0618e-13, 1.7786e-12, 1.0000e+00, 1.7933e-20]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(1, device='cuda:0') tensor([[3.6447e-01, 6.1946e-01, 2.3589e-05, 1.6047e-02]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(0, device='cuda:0') tensor([[0.4464, 0.3455, 0.0401, 0.1681]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(3, device='cuda:0') tensor([[2.6094e-03, 4.2404e-03, 4.5262e-07, 9.9315e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(3, device='cuda:0') tensor([[9.5329e-06, 3.7756e-05, 5.9504e-14, 9.9995e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(2, device='cuda:0') tensor([[6.5081e-08, 1.2288e-07, 1.0000e+00, 2.1677e-14]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(overlays):\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m         \u001b[43mwin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurfaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     64\u001b[0m pygame\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "\n",
    "import pygame\n",
    "\n",
    "# Constants\n",
    "WIN_WIDTH = 160 * 4\n",
    "WIN_HEIGHT = 210 * 4\n",
    "FRAMERATE = 50\n",
    "\n",
    "# Pygame Setup\n",
    "win = pygame.display.set_mode((WIN_WIDTH, WIN_HEIGHT))\n",
    "pygame.display.set_caption(\"Agent Testing\")\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "for test_point in test_data:\n",
    "    render = test_point['render']\n",
    "    state = torch.Tensor(test_point['state']).to(device)\n",
    "    shapley = model.infer(state).squeeze(0)\n",
    "\n",
    "    # zero_mask = state == 0\n",
    "    # print(zero_mask.shape)\n",
    "    # print(zero_mask.shape)\n",
    "    # print(zero_mask.squeeze(0).unsqueeze(-1).shape)\n",
    "    # shapley[zero_mask.squeeze(0), :] = 0\n",
    "\n",
    "    render_img = render.astype(np.uint8)\n",
    "    overlays = shapley_to_rgba_overlay(shapley)\n",
    "    # surfaces = [overlay_surfaces(render_img, overlay) for overlay in overlays]\n",
    "    # print(overlays[0].shape)\n",
    "    # pygame.surfarray.make_surface(overlays[0][..., :3])\n",
    "    surf = pygame.surfarray.make_surface(cv2.resize(render_img, (WIN_HEIGHT, WIN_WIDTH), cv2.INTER_NEAREST))\n",
    "    surfaces = [\n",
    "        pygame.surfarray.make_surface(cv2.resize(blend_overlay(cv2.resize(np.swapaxes(img, 0, 1), (210, 160)), render_img), (WIN_HEIGHT, WIN_WIDTH), cv2.INTER_NEAREST))\n",
    "        for img in overlays\n",
    "    ]\n",
    "\n",
    "    border = 0\n",
    "    # surfaces = [\n",
    "    #     pygame.surfarray.make_surface(cv2.resize(np.swapaxes(img[..., :3], 0, 1), (WIN_HEIGHT, WIN_WIDTH), cv2.INTER_NEAREST))\n",
    "    #     for img in overlays\n",
    "    # ]\n",
    "    print(model.target(state).argmax(), torch.softmax(model.target(state), dim=-1))\n",
    "    action = 0\n",
    "    while action < len(overlays):\n",
    "        dt = clock.tick(FRAMERATE) * 0.001\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                break\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_RETURN:\n",
    "                    action += 1\n",
    "                if event.key == pygame.K_RIGHT:\n",
    "                    action += 1\n",
    "                if event.key == pygame.K_LEFT:\n",
    "                    action = max(action - 1, 0)\n",
    "        if action >= len(overlays):\n",
    "            break\n",
    "\n",
    "        win.blit(surfaces[action], (0, 0))\n",
    "        pygame.display.update()\n",
    "\n",
    "pygame.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
